{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:43:34.290530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 21:43:35.170181: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-01 21:43:35.310622: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-01 21:43:35.310677: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-01 21:43:35.429218: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-01 21:43:37.503625: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 21:43:37.504041: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 21:43:37.504068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import awkward as aw\n",
    "import uproot\n",
    "from util import get_minit_from_procces_file, save_array_to_file, significance, get_cutted_files, ks_weighted\n",
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sort_jets import *\n",
    "from tensorflow.keras import layers\n",
    "import cloudpickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as conf_file:\n",
    "    config = yaml.load(conf_file, Loader=yaml.Loader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the cuted files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cut = 0\n",
    "with_VBF_cut = False\n",
    "if with_VBF_cut:\n",
    "    VBF_cut = '_with_VBF_cut'\n",
    "else:\n",
    "    VBF_cut = '_without_VBF_cut'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add sorted jets branches\n",
    "# # Run only once\n",
    "# for process in config['ttree']['files'].keys():\n",
    "#     temp_data = get_minit_from_procces_file(process, config, show_stat=False, do_cut=do_cut, with_VBF_cut=with_VBF_cut) #True)\n",
    "#     print(\"first part done\")\n",
    "#     temp_data[\"isVBF\"] = 1 if \"VBF\" in process  else 0\n",
    "#     temp_data[\"isGGF\"] = 1 if \"GGF\" in process  else 0\n",
    "#     # temp_data[\"weight\"] = temp_data[\"scale1fb\"] * temp_data[\"intLumi\"]\n",
    "#     # Add branches with sorted jets for rnn\n",
    "#     r = sort_jets(process, config)\n",
    "#     temp_data[\"jets_pt_sorted\"] =  aw.values_astype(aw.Array(r[\"jets_pt_sorted\"]), \"float32\") \n",
    "#     temp_data[\"jets_eta_sorted\"] = aw.values_astype(aw.Array(r[\"jets_eta_sorted\"]), \"float32\") \n",
    "#     temp_data[\"jets_phi_sorted\"] = aw.values_astype(aw.Array(r[\"jets_phi_sorted\"]), \"float32\") \n",
    "#     temp_data[\"jets_e_sorted\"] = aw.values_astype(aw.Array(r[\"jets_e_sorted\"]), \"float32\") \n",
    "#     new_branch = [\"isVBF\", \"isGGF\", \"jets_pt_sorted\",\"jets_eta_sorted\",\"jets_phi_sorted\",\"jets_e_sorted\"]\n",
    "#     if \"GGF\" in process:\n",
    "#         new_branch_ggf=[\"jet1_pt\", \"jet1_eta\", \"jet1_phi\", \"jet1_e\", \n",
    "#                         \"jet2_pt\", \"jet2_eta\", \"jet2_phi\", \"jet2_e\",\n",
    "#                         \"dphijj\", \"signetajj\", \"mjj\", \"detajj\",\n",
    "#                         \"njet30\", \"scale1fb\", \"intLumi\"]\n",
    "#         for branch in new_branch_ggf:\n",
    "#             temp_data[branch] = aw.values_astype(aw.Array(r[branch]), \"float32\")\n",
    "#         new_branch+=new_branch_ggf\n",
    "#     temp_data[\"weight\"] = temp_data[\"scale1fb\"] * temp_data[\"intLumi\"]\n",
    "#     save_array_to_file(temp_data, f\"test/{process}_{do_cut}_cuts{VBF_cut}.root\", config, extra_branch=new_branch)\n",
    "# del temp_data\n",
    "# del r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get cuted files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = uproot.LRUArrayCache(\"1 GB\")\n",
    "data_chain = uproot.lazy([f\"output/test/{procees}_{do_cut}_cuts{VBF_cut}.root:miniT\" for procees in config['ttree']['files'].keys()])#,array_cache=cache)\n",
    "# consider only events with weight>0\n",
    "data_s = aw.flatten(data_chain.mask[data_chain[\"isVBF\"] == True].mask[data_chain[\"weight\"]>=0], axis=0)\n",
    "# use only ggf events with njet30>1 for training and fitting\n",
    "data_b = aw.flatten(data_chain.mask[data_chain[\"isVBF\"] == False].mask[data_chain[\"weight\"]>=0], axis=0)\n",
    "# w=data_chain[\"weight\"]\n",
    "# cache\n",
    "# data_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491044\n",
      "signal: 10909.44921875, bkg: 6074543.5, significance: 4.422383172057975, asimov_significance: 4.425019382446251\n",
      "No cuts s/b= 0.0017959290416911244\n",
      "s_mc_events: 491044, b_mc_events: 4136843\n"
     ]
    }
   ],
   "source": [
    "s = aw.sum(data_s['weight'])\n",
    "b = aw.sum(data_b['weight'])\n",
    "s_e = len(data_s[\"weight\"])\n",
    "b_e = len(data_b)\n",
    "s_b = s/b\n",
    "# del data_s2\n",
    "print(aw.sum(data_s['isVBF']))\n",
    "print(f\"signal: {s}, bkg: {b}, significance: {s/math.sqrt(s+b)}, asimov_significance: {significance(s,b)}\")\n",
    "print(f\"No cuts s/b= {s_b}\")\n",
    "print(f\"s_mc_events: {s_e}, b_mc_events: {b_e}\")\n",
    "# cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frac = 0.7\n",
    "val_frac = 0.15\n",
    "array_len = len(data_s[\"weight\"])\n",
    "mask = np.zeros(array_len, dtype=int)\n",
    "mask1=mask.copy()\n",
    "mask2=mask.copy()\n",
    "mask3 = mask.copy()\n",
    "mask[:int(array_len * training_frac)] = 1\n",
    "mask[int(array_len * training_frac):int(array_len * (training_frac+val_frac))] = 2\n",
    "np.random.shuffle(mask)\n",
    "for i in range(len(mask)):\n",
    "    if mask[i]==1:\n",
    "        mask1[i] = 1\n",
    "    elif mask[i]==2:\n",
    "        mask2[i] = 1\n",
    "    elif mask[i]==0:\n",
    "        mask3[i] = 1\n",
    "del mask\n",
    "mask1 = aw.Array(mask1.astype(bool))\n",
    "mask2 = aw.Array(mask2.astype(bool))\n",
    "mask3 = aw.Array(mask3.astype(bool))\n",
    "s_training = aw.flatten(data_s.mask[mask1], axis=0)\n",
    "s_val = aw.flatten(data_s.mask[mask2],axis=0)\n",
    "s_test = aw.flatten(data_s.mask[mask3],axis=0)\n",
    "del mask1\n",
    "del mask2\n",
    "del mask3\n",
    "# cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_len = len(data_b[\"weight\"])\n",
    "mask = np.zeros(array_len, dtype=int)\n",
    "mask1=mask.copy()\n",
    "mask2=mask.copy()\n",
    "mask3 = mask.copy()\n",
    "mask[:int(array_len * training_frac)] = 1\n",
    "mask[int(array_len * training_frac):int(array_len * (training_frac+val_frac))] = 2\n",
    "np.random.shuffle(mask)\n",
    "for i in range(len(mask)):\n",
    "    if mask[i]==1:\n",
    "        mask1[i] = 1\n",
    "    elif mask[i]==2:\n",
    "        mask2[i] = 1\n",
    "    elif mask[i]==0:\n",
    "        mask3[i] = 1\n",
    "del mask\n",
    "mask1 = aw.Array(mask1.astype(bool))\n",
    "mask2 = aw.Array(mask2.astype(bool))\n",
    "mask3 = aw.Array(mask3.astype(bool))\n",
    "# use only ggf events with njet30>1 for training and fitting\n",
    "b_training = aw.flatten(data_b.mask[mask1].mask[data_b[\"njet30\"]>1], axis=0)\n",
    "b_val = aw.flatten(data_b.mask[mask2].mask[data_b[\"njet30\"]>1],axis=0)\n",
    "b_test = aw.flatten(data_b.mask[mask3].mask[data_b[\"njet30\"]>1],axis=0)\n",
    "# save events with njets<2 for statistics\n",
    "b_training_1_0_jets = aw.flatten(data_b.mask[mask1].mask[data_b[\"njet30\"]<=1], axis=0)\n",
    "b_val_1_0_jets = aw.flatten(data_b.mask[mask2].mask[data_b[\"njet30\"]<=1],axis=0)\n",
    "b_test_1_0_jets = aw.flatten(data_b.mask[mask3].mask[data_b[\"njet30\"]<=1],axis=0)\n",
    "\n",
    "del mask1\n",
    "del mask2\n",
    "del mask3\n",
    "# cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_training_1_0_jets = aw.to_numpy(b_training_1_0_jets[[\"njet30\", \"weight\"]])\n",
    "b_val_1_0_jets = aw.to_numpy(b_val_1_0_jets[[\"njet30\", \"weight\"]])\n",
    "b_test_1_0_jets = aw.to_numpy(b_test_1_0_jets[[\"njet30\", \"weight\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/cristobalchavez21/notebooks/production_mode_classifier/dpj/DPJAnalisis/output/train_test/\"\n",
    "np.save(path+\"b_training_1_0_jets\", b_training_1_0_jets)\n",
    "np.save(path+\"b_val_1_0_jets\", b_val_1_0_jets)\n",
    "np.save(path+\"b_test_1_0_jets\", b_test_1_0_jets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_inputs = [\"jets_pt_sorted\", \"jets_eta_sorted\", \"jets_phi_sorted\",\n",
    "                \"jets_e_sorted\"\n",
    "             ]\n",
    "# # cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_w_train = aw.to_numpy(s_training[\"weight\"])\n",
    "s_y_train = aw.to_numpy(s_training[\"isVBF\"])\n",
    "b_w_train = aw.to_numpy(b_training[\"weight\"])\n",
    "b_y_train = aw.to_numpy(b_training[\"isVBF\"])\n",
    "\n",
    "s_w_val = aw.to_numpy(s_val[\"weight\"])\n",
    "s_y_val = aw.to_numpy(s_val[\"isVBF\"])\n",
    "b_w_val = aw.to_numpy(b_val[\"weight\"])\n",
    "b_y_val = aw.to_numpy(b_val[\"isVBF\"])\n",
    "\n",
    "s_w_test = aw.to_numpy(s_test[\"weight\"])\n",
    "s_y_test = aw.to_numpy(s_test[\"isVBF\"])\n",
    "b_w_test = aw.to_numpy(b_test[\"weight\"])\n",
    "b_y_test = aw.to_numpy(b_test[\"isVBF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data\n",
    "Also reshapes for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum number of jets to be used in rnn\n",
    "max_jets = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:57:46.336016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-01 21:57:46.336236: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-01 21:57:46.336359: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MSI): /proc/driver/nvidia/version does not exist\n",
      "2022-12-01 21:57:46.337482: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jets_pt_sorted branch done\n",
      "jets_eta_sorted branch done\n",
      "jets_phi_sorted branch done\n",
      "jets_e_sorted branch done\n"
     ]
    }
   ],
   "source": [
    "# Zero and None padding of signal and background (training)\n",
    "norm_layers = []\n",
    "s_n = len(s_training)\n",
    "b_n = len(b_training)\n",
    "dt = []\n",
    "for branch in BDT_inputs:\n",
    "    dt.append((branch, float))\n",
    "s_x_train_norm = np.zeros((s_n,max_jets), dtype=dt)\n",
    "b_x_train_norm = np.zeros((b_n,max_jets), dtype=dt)\n",
    "\n",
    "for branch in BDT_inputs:\n",
    "# for branch in [\"jets_e_sorted\"]:\n",
    "#######################################\n",
    "###### ZERO PADDING ###################\n",
    "#######################################\n",
    "    # Signal\n",
    "    s_feature = s_training[branch]\n",
    "    # Zero padding\n",
    "    # Add zeros to have fix length sequences\n",
    "    s_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        s_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float')\n",
    "    # None padding for normalization\n",
    "    s_padded_inputs_none = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        s_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float', value=None)\n",
    "    # s_mask = embedding.compute_mask(s_padded_inputs)\n",
    "\n",
    "    # Background\n",
    "    b_feature = b_training[branch]\n",
    "    # Zero padding\n",
    "    # Add zeros to have fix length sequences\n",
    "    b_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        b_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float')\n",
    "    # None padding for normalization\n",
    "    b_padded_inputs_none = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        b_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float', value=None)\n",
    "    # b_mask = embedding.compute_mask(b_padded_inputs)\n",
    "\n",
    "    # Concatenate padded signal and bakcground to normalize considering all events\n",
    "    s_b_padded = np.concatenate((s_padded_inputs, b_padded_inputs))\n",
    "    # creating mask to ignore zeros in normalization\n",
    "    embedding = layers.Embedding(input_dim=5000, output_dim=19, mask_zero=True)\n",
    "    s_b_mask = embedding.compute_mask(s_b_padded)\n",
    "\n",
    "#################################################\n",
    "########## FITTING NORMALIZATION LAYER ##########\n",
    "#################################################\n",
    "    # normalization layer\n",
    "    norm_layer = layers.Normalization(axis=None)\n",
    "    # fitting normalization layer to signal+bkg ignoring zeros\n",
    "    norm_layer.adapt(tf.boolean_mask(s_b_padded, s_b_mask))\n",
    "    # save normalization layer to a list to use on later\n",
    "    norm_layers.append(norm_layer)\n",
    "\n",
    "#################################################\n",
    "######## NORMALIZING ############################\n",
    "#################################################\n",
    "\n",
    "    # normalize signal None padded features (normalizing zero padded would change the zeros)\n",
    "    s_normalized_f = norm_layer(s_padded_inputs_none)\n",
    "    # change nan back to zeros\n",
    "    s_normalized_f = np.nan_to_num(s_normalized_f.numpy()) \n",
    "    s_x_train_norm[branch] = s_normalized_f\n",
    "\n",
    "    # normalize background None padded features (normalizing zero padded would change the zeros)\n",
    "    b_normalized_f = norm_layer(b_padded_inputs_none)\n",
    "    # change nan back to zeros\n",
    "    b_normalized_f = np.nan_to_num(b_normalized_f.numpy()) \n",
    "    b_x_train_norm[branch] = b_normalized_f\n",
    "    print(f\"{branch} branch done\")\n",
    "s_x_train_norm = s_x_train_norm.view((float, len(BDT_inputs)))\n",
    "b_x_train_norm = b_x_train_norm.view((float, len(BDT_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mean': 65538.52, 'variance': 2910983000.0, 'axis': None},\n",
       " {'mean': 0.00801053, 'variance': 4.21058, 'axis': None},\n",
       " {'mean': 0.008975166, 'variance': 3.280814, 'axis': None},\n",
       " {'mean': 356531.2, 'variance': 257906570000.0, 'axis': None}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mean and variance for each feature and put them in a list of dic\n",
    "mean_var_list = []\n",
    "for i in range(len(BDT_inputs)):\n",
    "    # mean and variance are store as layer weights\n",
    "    layer_weights = norm_layers[i].get_weights()\n",
    "    dic = {\"mean\": layer_weights[0], \"variance\": layer_weights[1], \"axis\": None}\n",
    "    mean_var_list.append(dic)\n",
    "del dic\n",
    "del layer_weights\n",
    "mean_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of means and variances\n",
    "path = \"/home/cristobalchavez21/notebooks/production_mode_classifier/dpj/DPJAnalisis/output/train_test/\"\n",
    "with open(path+\"normalization_params.pkl\", \"wb\") as o_file:\n",
    "    cloudpickle.dump(mean_var_list, o_file)\n",
    "# cloudpickle.dump(scaler, open('scaler.pkl', 'wb'))<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how it should be loaded and used\n",
    "# norm_params = cloudpickle.load(open(f'{path}normalization_params.pkl', 'rb'))\n",
    "# new_layer = tf.keras.layers.Normalization(**norm_params[0])\n",
    "# new_layer(np.array([1,20,60000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array that contains sequence length of each event (number of jets used)\n",
    "s_seq_len_train = aw.to_numpy(s_training[\"njet30\"])\n",
    "s_seq_len_train[s_training[\"njet30\"]>max_jets] = max_jets\n",
    "b_seq_len_train = aw.to_numpy(b_training[\"njet30\"])\n",
    "b_seq_len_train[b_training[\"njet30\"]>max_jets] = max_jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "path = \"/home/cristobalchavez21/notebooks/production_mode_classifier/dpj/DPJAnalisis/output/train_test/\"\n",
    "save(path+\"signal_x_train_norm\", s_x_train_norm)\n",
    "save(path+\"bkg_x_train_norm\", b_x_train_norm)\n",
    "save(path+\"signal_w_train\", s_w_train)\n",
    "save(path+\"bkg_w_train\", b_w_train)\n",
    "save(path+\"signal_y_train\", s_y_train)\n",
    "save(path+\"bkg_y_train\", b_y_train)\n",
    "save(path+\"signal_seq_len_train\", s_seq_len_train)\n",
    "save(path+\"bkg_seq_len_train\", b_seq_len_train)\n",
    "del s_training\n",
    "del b_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jets_pt_sorted branch done\n",
      "jets_eta_sorted branch done\n",
      "jets_phi_sorted branch done\n",
      "jets_e_sorted branch done\n"
     ]
    }
   ],
   "source": [
    "# Zero and None padding of signal and background (validation)\n",
    "s_n = len(s_val)\n",
    "b_n = len(b_val)\n",
    "s_x_val_norm = np.zeros((s_n,max_jets), dtype=dt)\n",
    "b_x_val_norm = np.zeros((b_n,max_jets), dtype=dt)\n",
    "index=0\n",
    "for branch in BDT_inputs:\n",
    "#######################################\n",
    "###### ZERO PADDING ###################\n",
    "#######################################\n",
    "    # Signal\n",
    "    s_feature = s_val[branch]\n",
    "    # Zero padding\n",
    "    # Add zeros to have fix length sequences\n",
    "    s_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        s_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float')\n",
    "    # None padding for normalization\n",
    "    s_padded_inputs_none = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        s_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float', value=None)\n",
    "    # s_mask = embedding.compute_mask(s_padded_inputs)\n",
    "\n",
    "    # Background\n",
    "    b_feature = b_val[branch]\n",
    "    # Zero padding\n",
    "    # Add zeros to have fix length sequences\n",
    "    b_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        b_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float')\n",
    "    # None padding for normalization\n",
    "    b_padded_inputs_none = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        b_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float', value=None)\n",
    "    # b_mask = embedding.compute_mask(b_padded_inputs)\n",
    "\n",
    "    # Concatenate padded signal and bakcground to normalize considering all events\n",
    "    # s_b_padded = np.concatenate((s_padded_inputs, b_padded_inputs))\n",
    "    # creating mask to ignore zeros in normalization\n",
    "    # embedding = layers.Embedding(input_dim=5000, output_dim=19, mask_zero=True)\n",
    "    # s_b_mask = embedding.compute_mask(s_b_padded)\n",
    "\n",
    "#################################################\n",
    "########## FITTING NORMALIZATION LAYER ##########\n",
    "#################################################\n",
    "    # normalization layer (already fited to trainin data)\n",
    "    norm_layer = norm_layers[index]\n",
    "\n",
    "#################################################\n",
    "######## NORMALIZING ############################\n",
    "#################################################\n",
    "\n",
    "    # normalize signal None padded features (normalizing zero padded would change the zeros)\n",
    "    s_normalized_f = norm_layer(s_padded_inputs_none)\n",
    "    # change nan back to zeros\n",
    "    s_normalized_f = np.nan_to_num(s_normalized_f.numpy()) \n",
    "    s_x_val_norm[branch] = s_normalized_f\n",
    "\n",
    "    # normalize background None padded features (normalizing zero padded would change the zeros)\n",
    "    b_normalized_f = norm_layer(b_padded_inputs_none)\n",
    "    # change nan back to zeros\n",
    "    b_normalized_f = np.nan_to_num(b_normalized_f.numpy()) \n",
    "    b_x_val_norm[branch] = b_normalized_f\n",
    "    print(f\"{branch} branch done\")\n",
    "    index+=1\n",
    "s_x_val_norm = s_x_val_norm.view((float, len(BDT_inputs)))\n",
    "b_x_val_norm = b_x_val_norm.view((float, len(BDT_inputs)))\n",
    "del b_normalized_f\n",
    "del b_padded_inputs\n",
    "del b_padded_inputs_none\n",
    "del b_feature\n",
    "del s_feature\n",
    "del s_normalized_f\n",
    "del s_padded_inputs\n",
    "del s_padded_inputs_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array that contains sequence length of each event (number of jets used)\n",
    "s_seq_len_val = aw.to_numpy(s_val[\"njet30\"])\n",
    "s_seq_len_val[s_val[\"njet30\"]>max_jets] = max_jets\n",
    "b_seq_len_val = aw.to_numpy(b_val[\"njet30\"])\n",
    "b_seq_len_val[b_val[\"njet30\"]>max_jets] = max_jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(path+\"signal_x_val_norm\", s_x_val_norm)\n",
    "save(path+\"bkg_x_val_norm\", b_x_val_norm)\n",
    "save(path+\"signal_w_val\", s_w_val)\n",
    "save(path+\"bkg_w_val\", b_w_val)\n",
    "save(path+\"signal_y_val\", s_y_val)\n",
    "save(path+\"bkg_y_val\", b_y_val)\n",
    "save(path+\"signal_seq_len_val\", s_seq_len_val)\n",
    "save(path+\"bkg_seq_len_val\", b_seq_len_val)\n",
    "del s_val\n",
    "del b_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jets_pt_sorted branch done\n",
      "jets_eta_sorted branch done\n",
      "jets_phi_sorted branch done\n",
      "jets_e_sorted branch done\n"
     ]
    }
   ],
   "source": [
    "# Zero and None padding of signal and background (test)\n",
    "s_n = len(s_test)\n",
    "b_n = len(b_test)\n",
    "s_x_test_norm = np.zeros((s_n,max_jets), dtype=dt)\n",
    "b_x_test_norm = np.zeros((b_n,max_jets), dtype=dt)\n",
    "index=0\n",
    "for branch in BDT_inputs:\n",
    "#######################################\n",
    "###### ZERO PADDING ###################\n",
    "#######################################\n",
    "    # Signal\n",
    "    s_feature = s_test[branch]\n",
    "    # Zero padding\n",
    "    # Add zeros to have fix length sequences\n",
    "    s_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        s_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float')\n",
    "    # None padding for normalization\n",
    "    s_padded_inputs_none = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        s_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float', value=None)\n",
    "    # s_mask = embedding.compute_mask(s_padded_inputs)\n",
    "\n",
    "    # Background\n",
    "    b_feature = b_test[branch]\n",
    "    # Zero padding\n",
    "    # Add zeros to have fix length sequences\n",
    "    b_padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        b_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float')\n",
    "    # None padding for normalization\n",
    "    b_padded_inputs_none = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        b_feature, padding=\"post\", truncating=\"post\", maxlen=max_jets, dtype='float', value=None)\n",
    "    # b_mask = embedding.compute_mask(b_padded_inputs)\n",
    "\n",
    "    # Concatenate padded signal and bakcground to normalize considering all events\n",
    "    # s_b_padded = np.concatenate((s_padded_inputs, b_padded_inputs))\n",
    "    # creating mask to ignore zeros in normalization\n",
    "    # embedding = layers.Embedding(input_dim=5000, output_dim=19, mask_zero=True)\n",
    "    # s_b_mask = embedding.compute_mask(s_b_padded)\n",
    "\n",
    "#################################################\n",
    "########## FITTING NORMALIZATION LAYER ##########\n",
    "#################################################\n",
    "    # normalization layer (already fited to trainin data)\n",
    "    norm_layer = norm_layers[index]\n",
    "\n",
    "#################################################\n",
    "######## NORMALIZING ############################\n",
    "#################################################\n",
    "\n",
    "    # normalize signal None padded features (normalizing zero padded would change the zeros)\n",
    "    s_normalized_f = norm_layer(s_padded_inputs_none)\n",
    "    # change nan back to zeros\n",
    "    s_normalized_f = np.nan_to_num(s_normalized_f.numpy()) \n",
    "    s_x_test_norm[branch] = s_normalized_f\n",
    "\n",
    "    # normalize background None padded features (normalizing zero padded would change the zeros)\n",
    "    b_normalized_f = norm_layer(b_padded_inputs_none)\n",
    "    # change nan back to zeros\n",
    "    b_normalized_f = np.nan_to_num(b_normalized_f.numpy()) \n",
    "    b_x_test_norm[branch] = b_normalized_f\n",
    "    print(f\"{branch} branch done\")\n",
    "    index+=1\n",
    "s_x_test_norm = s_x_test_norm.view((float, len(BDT_inputs)))\n",
    "b_x_test_norm = b_x_test_norm.view((float, len(BDT_inputs)))\n",
    "del b_normalized_f\n",
    "del b_padded_inputs\n",
    "del b_padded_inputs_none\n",
    "del b_feature\n",
    "del s_feature\n",
    "del s_normalized_f\n",
    "del s_padded_inputs\n",
    "del s_padded_inputs_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array that contains sequence length of each event (number of jets used)\n",
    "s_seq_len_test = aw.to_numpy(s_test[\"njet30\"])\n",
    "s_seq_len_test[s_test[\"njet30\"]>max_jets] = max_jets\n",
    "b_seq_len_test = aw.to_numpy(b_test[\"njet30\"])\n",
    "b_seq_len_test[b_test[\"njet30\"]>max_jets] = max_jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(path+\"signal_x_test_norm\", s_x_test_norm)\n",
    "save(path+\"bkg_x_test_norm\", b_x_test_norm)\n",
    "save(path+\"signal_w_test\", s_w_test)\n",
    "save(path+\"bkg_w_test\", b_w_test)\n",
    "save(path+\"signal_y_test\", s_y_test)\n",
    "save(path+\"bkg_y_test\", b_y_test)\n",
    "save(path+\"signal_seq_len_test\", s_seq_len_test)\n",
    "save(path+\"bkg_seq_len_test\", b_seq_len_test)\n",
    "del s_test\n",
    "del b_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
